{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SophieShin/NLP_22_Fall/blob/main/%5BSSH%5Dlab04_nlp_embedding_tokeniser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMeJSwyhZOB5"
      },
      "source": [
        "# 1. Word Embeddings\n",
        "Let's get a basic understanding of word tokenisation, i.e. representing a word with a unique numeric token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XI1rONSjzqo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "add8a2fc-7aac-42a0-ce4d-54a686066990"
      },
      "source": [
        "corpus = \"hello hello there new world\"\n",
        "words = corpus.split()\n",
        "print(words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', 'hello', 'there', 'new', 'world']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Qd70aOekGHB"
      },
      "source": [
        "Manually construct a simple dictionary containing the words in the sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XVMiAeDkM5I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70a07709-5378-4b9e-e678-64bc321952f5"
      },
      "source": [
        "word_dict = {\"hello\":0, \"new\":1, \"there\":2, \"world\":3 }\n",
        "word_dict\n",
        "\n",
        "# More common usage:\n",
        "# word_dict = {word: idx for idx, word in enumerate(sorted(set(words)))} # sorted 해도 되고 안해도 되고 #순서만 차이\n",
        "# More common name for word_dict: word2idx"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'hello': 0, 'new': 1, 'there': 2, 'world': 3}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5A-pwn1lzzk"
      },
      "source": [
        "Get the index for each word in the sentence by looking it up in the dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sf9x-h9nl3BH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41f250ba-a55d-440a-9cd8-2f86c58ee9d2"
      },
      "source": [
        "indices = [word_dict[w] for w in words]\n",
        "indices"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0, 2, 1, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiHbuuFSmqGy"
      },
      "source": [
        "Convert the word indices of the input sentence to a tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rn472I0_msDs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d24cdb36-9158-459e-bf50-2ac4b73abc6a"
      },
      "source": [
        "import torch\n",
        "\n",
        "input_tensor = torch.LongTensor(indices)  # normally Long used for words\n",
        "\n",
        "input_tensor"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3, 3, 2, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Embedding Layer\n",
        "The embedding layer is a 2-D matrix of shape `(n_vocab x embedding_dimension)`. If we want to get the embeddings (word vectors) for a particular sentence, supply a list of indices of the sentence as input to the embedding layer. Each index in the input list maps to the specific row of the embedding layer matrix (word vector). The output shape after applying the input list of indices to the embedding layer is another 2-D matrix of shape `(n_words x embedding_dimension)`."
      ],
      "metadata": {
        "id": "kgok2qUPv_a7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irdq0r5QZVWx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cd6f7e0-a71d-4fe6-b5c1-7ad37c265fe6"
      },
      "source": [
        "import torch\n",
        "from torch.nn import Embedding\n",
        "\n",
        "# Create the embeddings for the word list that we have: \"hello hello there new world\"\n",
        "num_embeddings = len(word_dict) # num_embeddings: length of vocab\n",
        "embedding_dim = 3 # embedding_dim: size of embedding vector for each word\n",
        "\n",
        "emb_layer = Embedding(num_embeddings, embedding_dim) \n",
        "embeddings = emb_layer(input_tensor)\n",
        "\n",
        "print(f\"Embedding vector: {embeddings}\")\n",
        "print(f\"Vector shape: {embeddings.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding vector: tensor([[ 0.6226,  1.1489, -0.5202],\n",
            "        [ 0.6226,  1.1489, -0.5202],\n",
            "        [-0.9242,  0.1120, -1.1592],\n",
            "        [ 2.2501,  0.0472, -0.3660],\n",
            "        [-1.6560,  0.5086, -0.0907]], grad_fn=<EmbeddingBackward0>)\n",
            "Vector shape: torch.Size([5, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxZwlSzAtRLb"
      },
      "source": [
        "# Q1. Why are the embeddings the same for the first two rows?\n",
        "# A1. Since the first two rows(words) are the same."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The PyTorch built-in `Embedding` layer comes with randomly initialized weights that are updated with gradient descent as your model learns to map input indices to some kind of output. "
      ],
      "metadata": {
        "id": "DYP3otdxxhJ_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Mzn6jo4rBcV"
      },
      "source": [
        "### Exercise\n",
        "Create embeddings for the sentence `\"hello world\"` by a passing suitable parameters to the Embedding class. \n",
        "- you should look up the indices for the words in this sentence in `word_dict` and convert it to a tensor\n",
        "- each word should be converted into a 12-dimensional vector\n",
        "- print the embeddings for this sentence and its shape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdwBzOzarf4V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef02cf34-6a65-43f3-feb4-0fc563dc32cb"
      },
      "source": [
        "\n",
        "# Q2. Insert your code here\n",
        "words2 = 'hello world'.split()\n",
        "idx2 = [word_dict[w] for w in words2]\n",
        "input_tensor2 = torch.LongTensor(idx2)\n",
        "\n",
        "embeddings2 = emb_layer(input_tensor2)\n",
        "print(embeddings2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.6226,  1.1489, -0.5202],\n",
            "        [-1.6560,  0.5086, -0.0907]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbsNviD-bX0a"
      },
      "source": [
        "### Padding\n",
        "- Out-of-vocab or Unknown words should be padded (don't care about their gradients)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOTpgN8KbwDK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bf508fb-e28f-410c-bf54-371ea9049d39"
      },
      "source": [
        "num_embs = 6\n",
        "emb_dim = 4\n",
        "\n",
        "emb2 = Embedding(num_embs, emb_dim, padding_idx=3)\n",
        "emb2.weight"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 0.1930, -0.4679,  0.8713, -1.3061],\n",
              "        [-0.1835, -1.3394, -0.5989, -0.1533],\n",
              "        [ 1.1934, -0.8885, -0.2454,  1.1304],\n",
              "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.4060, -2.0562, -0.6274,  0.6245],\n",
              "        [ 1.3420, -0.7377,  0.2546, -0.7532]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmhI-1ZCcBnZ"
      },
      "source": [
        "# Q3. What is the effect of applying padding_idx=3 to the embedding?\n",
        "# A3. If this embedding is passed through the Neural Network, then the 3rd embedding will be ignored when calculating in the network."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-trained Embeddings\n",
        "Often it is better to use pretrained embeddings that do not update but instead are frozen. [GloVe](https://nlp.stanford.edu/projects/glove/) embeddings are one of the most popular pretrained word embeddings in use. The best performing embeddings are their Common Crawl embeddings with 840B tokens; however, they take very long to download. We'll download the Wikipedia embeddings with 6B tokens.\n",
        "\n",
        "[Source](https://github.com/A-Jacobson/CNN_Sentence_Classification/blob/master/WordVectors.ipynb)"
      ],
      "metadata": {
        "id": "BXiS49ltxmJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Takes 3+ mins\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip \n",
        "!unzip glove.6B.zip\n",
        "!ls -lat"
      ],
      "metadata": {
        "id": "gxme5dezOHTs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6f46f7f-70e6-420e-b756-a3f68556fe8d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-09-17 06:07:30--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2022-09-17 06:07:30--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2022-09-17 06:07:30--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.01MB/s    in 2m 39s  \n",
            "\n",
            "2022-09-17 06:10:09 (5.17 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n",
            "total 3039148\n",
            "drwxr-xr-x 1 root root       4096 Sep 17 06:10 .\n",
            "drwxr-xr-x 1 root root       4096 Sep 17 05:01 ..\n",
            "drwxr-xr-x 1 root root       4096 Sep 14 13:44 sample_data\n",
            "drwxr-xr-x 4 root root       4096 Sep 14 13:43 .config\n",
            "-rw-r--r-- 1 root root  862182613 Oct 25  2015 glove.6B.zip\n",
            "-rw-rw-r-- 1 root root 1037962819 Aug 27  2014 glove.6B.300d.txt\n",
            "-rw-rw-r-- 1 root root  171350079 Aug  4  2014 glove.6B.50d.txt\n",
            "-rw-rw-r-- 1 root root  693432828 Aug  4  2014 glove.6B.200d.txt\n",
            "-rw-rw-r-- 1 root root  347116733 Aug  4  2014 glove.6B.100d.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the embeddings with 50 dimensions contained in `glove.6B.50d.txt`"
      ],
      "metadata": {
        "id": "unE4rDqRW4hy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "AghzBA7UUSfM"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper Function: create Glove dictionary\n",
        "`load_glove()` reads the glove embeddings txt file line by line and creates a dictionary mapping words to vectors. For `glove.6B.50d.txt` this dictionary has 400k words each mapped to a 50 dimensional vector. We can use this to check the values of our pytorch embedding layer."
      ],
      "metadata": {
        "id": "XQHtODaMUq0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 이 부분 잘 이해가 안 됨\n",
        "def load_glove(path):\n",
        "    \"\"\"\n",
        "    creates a dictionary mapping words to vectors from a file in glove format.\n",
        "    \"\"\"\n",
        "    with open(path) as f:\n",
        "        glove = {}\n",
        "        for line in f.readlines():\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.array(values[1:], dtype='float32')\n",
        "            glove[word] = vector\n",
        "        return glove"
      ],
      "metadata": {
        "id": "IkKFFSyHQ1YH"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper Function: get embeddings for a sentence\n",
        "`load_glove_embeddings` takes a dictionary mapping words to indexes (must be computed from your training corpus) and returns a matrix of embeddings which we can use to initialize a Pytorch embedding layer."
      ],
      "metadata": {
        "id": "fn6ySixbUf84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 이 부분 잘 이해가 안 됨\n",
        "def load_glove_embeddings(path, word2idx, embedding_dim=50):\n",
        "    with open(path) as f:\n",
        "        embeddings = np.zeros((len(word2idx), embedding_dim))\n",
        "        for line in f.readlines():\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            index = word2idx.get(word)\n",
        "            if index:\n",
        "                vector = np.array(values[1:], dtype='float32')\n",
        "                embeddings[index] = vector\n",
        "        return torch.from_numpy(embeddings).float()"
      ],
      "metadata": {
        "id": "JJDhbIEvQ8ut"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glove_path = 'glove.6B.50d.txt'\n",
        "%time glove = load_glove(glove_path)  ## %time : wall time, %%time : CPU time"
      ],
      "metadata": {
        "id": "Vq1dgYbyRAfC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fa85e8f-4674-4b10-d489-fd0cb2d7a164"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 4.17 s, sys: 590 ms, total: 4.76 s\n",
            "Wall time: 4.77 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(glove)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPTcFIIEE3Ho",
        "outputId": "254df5a1-b0f7-4c21-be13-3da4136b3533"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400000"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Toy Example"
      ],
      "metadata": {
        "id": "k6kniSXNVLVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = 'the cow jumped over the moon.'\n",
        "vocab = set(corpus.split()) # compute vocab, 6 words\n",
        "word2idx = {word: idx for idx, word in enumerate(vocab)} # create word index"
      ],
      "metadata": {
        "id": "yF2sJ4S7RWyP"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16kjjQLHGWIa",
        "outputId": "7cea1610-2cdd-47f2-d901-8d86a5719292"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cow', 'jumped', 'moon.', 'over', 'the'}"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word2idx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfPccspL2o_2",
        "outputId": "f1b51a59-1d9e-4016-fa98-34b5415043ae"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'the': 0, 'jumped': 1, 'moon.': 2, 'cow': 3, 'over': 4}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "toy_embeddings = load_glove_embeddings(glove_path, word2idx)\n",
        "# 5 words x 50 embedding dimensions\n",
        "toy_embeddings.shape"
      ],
      "metadata": {
        "id": "9DE1vrosR7kE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb3903c3-171d-4228-b4ad-57090c189713"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 50])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn.Parameter(toy_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hg3ZNKmn3jn7",
        "outputId": "fd965713-cbe7-449d-de6f-9c75b88c8c9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000],\n",
              "        [ 0.6125, -0.4817, -0.7420, -0.5520, -0.0076,  1.6101, -0.8856, -0.8198,\n",
              "          1.5144, -0.2280,  0.5537, -0.1839,  0.7049, -0.3693,  1.0668,  1.1077,\n",
              "          0.1971,  0.2473, -0.6840,  0.5475, -0.0383, -0.7899,  0.6113,  0.3147,\n",
              "          0.5021, -1.6535, -0.4278,  1.0404,  0.2943, -0.3689,  1.3148, -0.1844,\n",
              "          0.0928,  0.7757, -0.5484, -0.1464,  0.5113,  0.0472,  0.4178, -0.1832,\n",
              "         -0.4420, -0.2524, -0.3359,  0.3096,  1.9192,  0.3396, -0.2734, -0.0132,\n",
              "          0.6497, -0.8586],\n",
              "        [-0.5146, -0.5915,  1.6021,  0.1343,  0.8857,  0.1214, -1.1581, -0.5010,\n",
              "          0.0531,  0.3792,  0.4452, -0.7906, -0.5816, -0.3144,  0.3536, -0.0498,\n",
              "         -0.9605, -0.2171, -1.2903, -0.4726,  0.6284,  0.3652,  0.0310, -0.8863,\n",
              "         -0.1141, -0.8959,  0.5745,  0.3492,  0.1862,  0.0042,  2.1329,  0.6074,\n",
              "          1.0759,  0.9483,  0.1163, -0.9871, -0.5003, -0.0374,  0.9105, -0.5753,\n",
              "         -0.5781, -0.1867,  0.6017,  0.0042,  0.0662, -0.4657, -0.0542, -0.4960,\n",
              "          0.4931, -0.5721],\n",
              "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000],\n",
              "        [ 0.1297,  0.0881,  0.2438,  0.0781, -0.1278,  0.2783, -0.4869,  0.1965,\n",
              "         -0.3956, -0.2836, -0.4742, -0.5932, -0.5880, -0.3170,  0.4959,  0.0088,\n",
              "          0.0396, -0.4250, -0.9764, -0.4653,  0.0207,  0.0860,  0.3932, -0.5125,\n",
              "         -0.1791, -1.8333,  0.5622,  0.4163,  0.0751,  0.0219,  3.7840,  0.7107,\n",
              "         -0.0739,  0.1537, -0.3853, -0.0702, -0.3537,  0.0745, -0.0842, -0.4555,\n",
              "         -0.0811,  0.3916,  0.1730,  0.2254, -0.1284,  0.4095, -0.2608,  0.0909,\n",
              "         -0.6051, -0.9827]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "toy_embeddings.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SErR3YijF7VM",
        "outputId": "87dae438-48af-4281-f1ab-bca411c34e69"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 50])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "toy_embedding = Embedding(toy_embeddings.size(0), toy_embeddings.size(1)) # 5*50 embedding layer를 생성\n",
        "toy_embedding.weight = nn.Parameter(toy_embeddings) #glove를 이용한 embedding parameter를 카피\n",
        "toy_embedding "
      ],
      "metadata": {
        "id": "Hc-lpw1_SV_g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "717bea92-ac3c-4098-cacd-a63a7c3d4067"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(5, 50)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the embedding for \"cow\""
      ],
      "metadata": {
        "id": "X162-a5xX2wN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx = word2idx['cow']\n",
        "toy_embedding(Variable(torch.LongTensor([idx])))\n",
        "\n",
        "# Tensor와 Variable은 2018년에 합쳐진 class로 이제는 Tensor로 통합되었다. \n",
        "# 기존에는 Variable에서 gradient를 자동으로 계산해주는 역할을 해주었지만, \n",
        "# 이제는 Tensor가 그 기능을 할 수 있게 되었다.\n",
        "# 즉, Variable을 사용할 수는 있지만 Tensor로 return이 되니 굳이 사용할 필요없는 클래스이다.\n",
        "# 즉, requires_grad로 gradient 계산 여부를 Tensor로 사용할 수 있다."
      ],
      "metadata": {
        "id": "sc9H5nIES3zr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccc81f7a-ab78-475c-b028-31ec556a3a9a"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.6125, -0.4817, -0.7420, -0.5520, -0.0076,  1.6101, -0.8856, -0.8198,\n",
              "          1.5144, -0.2280,  0.5537, -0.1839,  0.7049, -0.3693,  1.0668,  1.1077,\n",
              "          0.1971,  0.2473, -0.6840,  0.5475, -0.0383, -0.7899,  0.6113,  0.3147,\n",
              "          0.5021, -1.6535, -0.4278,  1.0404,  0.2943, -0.3689,  1.3148, -0.1844,\n",
              "          0.0928,  0.7757, -0.5484, -0.1464,  0.5113,  0.0472,  0.4178, -0.1832,\n",
              "         -0.4420, -0.2524, -0.3359,  0.3096,  1.9192,  0.3396, -0.2734, -0.0132,\n",
              "          0.6497, -0.8586]], grad_fn=<EmbeddingBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check that it is the same as Glove's vector"
      ],
      "metadata": {
        "id": "MGL0d41RX69Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "glove.get('cow') # 'get'이라는 단어의 glove에서의 embedding"
      ],
      "metadata": {
        "id": "pIwbV9glTqSI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ad60408-2f5f-47f0-b807-ff6f965d5def"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.61253 , -0.48167 , -0.74199 , -0.55203 , -0.007596,  1.6101  ,\n",
              "       -0.88565 , -0.81981 ,  1.5144  , -0.22804 ,  0.55367 , -0.18392 ,\n",
              "        0.7049  , -0.36931 ,  1.0668  ,  1.1077  ,  0.19709 ,  0.24731 ,\n",
              "       -0.68395 ,  0.5475  , -0.038255, -0.78989 ,  0.61131 ,  0.31473 ,\n",
              "        0.50215 , -1.6535  , -0.42782 ,  1.0404  ,  0.29429 , -0.36889 ,\n",
              "        1.3148  , -0.18443 ,  0.092753,  0.77572 , -0.54845 , -0.14645 ,\n",
              "        0.51128 ,  0.047248,  0.41781 , -0.18324 , -0.44197 , -0.25237 ,\n",
              "       -0.3359  ,  0.3096  ,  1.9192  ,  0.3396  , -0.27341 , -0.01316 ,\n",
              "        0.64974 , -0.85857 ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Working with `torchtext` Tokenizer\n",
        "First: \n",
        "  - Install compatible versions of `torch` and `torchtext`\n",
        "  - Install `torchdata`\n",
        "  - Restart Runtime if required (`Runtime-->Restart runtime` or click on the RESTART RUNTIME button in the code cell)"
      ],
      "metadata": {
        "id": "6VVyCu-yVnLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.11.0 torchtext==0.12.0 torchdata"
      ],
      "metadata": {
        "id": "C6PVJg-WNobO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 987
        },
        "outputId": "625bec5b-25c9-4e33-ea48-d1947e75a476"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch==1.11.0\n",
            "  Downloading torch-1.11.0-cp37-cp37m-manylinux1_x86_64.whl (750.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 750.6 MB 9.1 kB/s \n",
            "\u001b[?25hCollecting torchtext==0.12.0\n",
            "  Downloading torchtext-0.12.0-cp37-cp37m-manylinux1_x86_64.whl (10.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.4 MB 24.4 MB/s \n",
            "\u001b[?25hCollecting torchdata\n",
            "  Downloading torchdata-0.4.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 38.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.11.0) (4.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.12.0) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.12.0) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.12.0) (2.23.0)\n",
            "Collecting portalocker>=2.0.0\n",
            "  Downloading portalocker-2.5.1-py2.py3-none-any.whl (15 kB)\n",
            "Collecting torchdata\n",
            "  Downloading torchdata-0.4.0-cp37-cp37m-manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 32.0 MB/s \n",
            "\u001b[?25h  Downloading torchdata-0.3.0-py3-none-any.whl (47 kB)\n",
            "\u001b[K     |████████████████████████████████| 47 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting urllib3>=1.25\n",
            "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 62.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.12.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.12.0) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.12.0) (3.0.4)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 54.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: urllib3, torch, torchtext, torchdata\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.1+cu113\n",
            "    Uninstalling torch-1.12.1+cu113:\n",
            "      Successfully uninstalled torch-1.12.1+cu113\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.13.1\n",
            "    Uninstalling torchtext-0.13.1:\n",
            "      Successfully uninstalled torchtext-0.13.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 1.11.0 which is incompatible.\n",
            "torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.11.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.11.0 torchdata-0.3.0 torchtext-0.12.0 urllib3-1.25.11\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "View the versions installed (**AFTER** restarting runtime)"
      ],
      "metadata": {
        "id": "BIVsNqfPdlbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 show torch # should be 1.11.0"
      ],
      "metadata": {
        "id": "panDYMjlPHm_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b470c09-edab-4630-91e9-d9179dbb105d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: torch\n",
            "Version: 1.11.0\n",
            "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
            "Home-page: https://pytorch.org/\n",
            "Author: PyTorch Team\n",
            "Author-email: packages@pytorch.org\n",
            "License: BSD-3\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: typing-extensions\n",
            "Required-by: torchvision, torchtext, torchdata, torchaudio, fastai\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 show torchtext # should be 0.12.0"
      ],
      "metadata": {
        "id": "-mL1dLeEhfuY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ec36189-3fb0-4fde-f36d-cc49e330a9d4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: torchtext\n",
            "Version: 0.12.0\n",
            "Summary: Text utilities and datasets for PyTorch\n",
            "Home-page: https://github.com/pytorch/text\n",
            "Author: PyTorch core devs and James Bradbury\n",
            "Author-email: jekbradbury@gmail.com\n",
            "License: BSD\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: torch, requests, tqdm, numpy\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1. Small corpus of manually created sentences\n",
        "- `torchtext` has several tokenisers, but \"basic english\" should be enough for this simple example\n",
        "- The vocabulary (`vocab`) is a dictionary containing the mapping between each token word to its corresponding token (index)"
      ],
      "metadata": {
        "id": "XpaxTDnC2CbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "sample_text = [\"The quick brown FOX jumps over the lazy dog.\",\n",
        "               \"A wizard's job: is to vex chumps quickly in fog?\",\n",
        "               \"Brown jars prevented the mixture from freezing too quickly...\",\n",
        "               \"How vexingly quick daft zebras jump!\",\n",
        "               \"When 'zombies' arrive, quickly fax Judge Pat ;-)\"]\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "def yield_tokens():\n",
        "  for s in sample_text:\n",
        "    tokens = tokenizer(s)\n",
        "    print(tokens)\n",
        "    yield tokens\n",
        "\n",
        "# yield, return 차이\n",
        "# return은 list 등을 반환, yeild는 generator를 반환\n",
        "# 제너레이터는 여러 개의 데이터를 미리 만들어 놓지 않고 필요할 때마다 즉석해서 하나씩 만들어낼 수 있는 객체\n",
        "# 이터러블한 것들은 우리가 원하는 만큼 접근해서 사용할 수 있기 때문에 매우 유용한 한편 이렇게 하기 위해 \n",
        "# 모든 값을 메모리에 담고 있어야 하기 때문에 큰 값을 다룰 때에는 별로 좋지 않습니다.\n",
        "# 제너레이터(generators)는 이터레이터(iterators)입니다. 하지만 제너레이터는 모든 값을 메모리에 담고 있지 않고 \n",
        "# 그때그때 값을 생성(generator)해서 반환하기 때문에 제너레이터를 사용할 때에는 \n",
        "# 한 번에 한 개의 값만 순환(iterate) 할 수 있습니다:\n",
        "\n",
        "token_generator = yield_tokens() # can now iterate through token_generator\n",
        "\n",
        "vocab = build_vocab_from_iterator(token_generator, specials= [\"<unk>\"]) # add special token \"<unk\"> 'unknown'\n",
        "vocab.set_default_index(vocab[\"<unk>\"]) # the index will be returned when OOV token is queried\n"
      ],
      "metadata": {
        "id": "nkDjdmbm0590",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e7af942-85f7-49b2-8427-c42067dfd71d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n",
            "['a', 'wizard', \"'\", 's', 'job', 'is', 'to', 'vex', 'chumps', 'quickly', 'in', 'fog', '?']\n",
            "['brown', 'jars', 'prevented', 'the', 'mixture', 'from', 'freezing', 'too', 'quickly', '.', '.', '.']\n",
            "['how', 'vexingly', 'quick', 'daft', 'zebras', 'jump', '!']\n",
            "['when', \"'\", 'zombies', \"'\", 'arrive', ',', 'quickly', 'fax', 'judge', 'pat', '-', ')']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### build_vocab_from_iterator 함수 이해\n",
        "torchtext.vocab.build_vocab_from_iterator(iterator: Iterable, min_freq: int = 1, specials: Optional[List[str]] = None, special_first: bool = True, max_tokens: Optional[int] = None) → torchtext.vocab.vocab.Vocab\n",
        "\n",
        "Build a Vocab from an iterator.\n",
        "Return Vocab Object\n",
        "\n",
        "---\n",
        "parameters\n",
        "\n",
        "*iterator – Iterator used to build Vocab. Must yield list or iterator of tokens.\n",
        "\n",
        "*min_freq – The minimum frequency needed to include a token in the vocabulary.\n",
        "\n",
        "*specials – Special symbols to add. The order of supplied tokens will be preserved.\n",
        "\n",
        "*special_first – Indicates whether to insert symbols at the beginning or at the end.\n",
        "\n",
        "*max_tokens – If provided, creates the vocab from the max_tokens - len(specials) most frequent tokens.\n",
        "\n",
        "[link text](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.build_vocab_from_iterator)\n",
        "torch doc site 참조\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aoOq5bD9MCkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q4. Based on the vocab, state three things that this tokenizer (torchtext \"basic_english\") does\n",
        "# apart from splitting by space\n",
        "# A4. The basic_english tokenizer does the below tasks in order to process further. \n",
        "# 1. Convert source text to lower case\n",
        "# 2. add space before and after single-quote, period, comma,left paren, right paren, exclam, question mark.\n",
        "# 3. replace colon, semicolon, (br /) with a space\n",
        "# 4. remove double-quote\n",
        "# 5. split on whitespace\n",
        "# Reference: https://jamesmccaffrey.wordpress.com/2021/06/23/tokenizing-text-using-the-basic-english-algorithm/\n"
      ],
      "metadata": {
        "id": "OhcHaNZii5oQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert strings in vocab to indexes"
      ],
      "metadata": {
        "id": "sqNbqBII7bEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# String to index (stoi) # Dictionary mapping tokens to indices.\n",
        "\n",
        "vocab.get_stoi()\n",
        "\n",
        "# Index to string (itos)\n",
        "# vocab.get_itos()"
      ],
      "metadata": {
        "id": "-Z23tLbQ6jX7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e9b7c97-1793-4799-8719-c55ab252f78a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'zombies': 43,\n",
              " 'zebras': 42,\n",
              " 'wizard': 41,\n",
              " 'when': 40,\n",
              " 'vexingly': 39,\n",
              " 'vex': 38,\n",
              " 'too': 37,\n",
              " 'to': 36,\n",
              " 'jars': 25,\n",
              " 'a': 12,\n",
              " 'chumps': 14,\n",
              " 'quick': 6,\n",
              " 'arrive': 13,\n",
              " 'is': 24,\n",
              " 'job': 26,\n",
              " 'freezing': 20,\n",
              " 'pat': 33,\n",
              " 'fox': 19,\n",
              " '!': 7,\n",
              " 'judge': 27,\n",
              " '<unk>': 0,\n",
              " \"'\": 2,\n",
              " 'fog': 18,\n",
              " ',': 9,\n",
              " 'mixture': 31,\n",
              " '-': 10,\n",
              " 's': 35,\n",
              " 'quickly': 3,\n",
              " ')': 8,\n",
              " '.': 1,\n",
              " 'brown': 5,\n",
              " 'how': 22,\n",
              " 'jump': 28,\n",
              " 'the': 4,\n",
              " 'dog': 16,\n",
              " 'fax': 17,\n",
              " 'in': 23,\n",
              " 'daft': 15,\n",
              " 'from': 21,\n",
              " 'prevented': 34,\n",
              " '?': 11,\n",
              " 'jumps': 29,\n",
              " 'lazy': 30,\n",
              " 'over': 32}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilities on Individual Tokens and Indexes"
      ],
      "metadata": {
        "id": "NdD0P5Ly8ZrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab['jump']"
      ],
      "metadata": {
        "id": "sju2q8Rb8Saf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52466643-b83f-40ea-ab5b-3946335f392a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Could get indices of multiple tokens put in a list"
      ],
      "metadata": {
        "id": "4nrSTzkCaJEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab.lookup_indices([\"wizard\", \"fox\"])"
      ],
      "metadata": {
        "id": "fbdjEiGw9cOz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9703b347-b92d-4d71-d410-bd23be2a2667"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[41, 19]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Could look up tokens from a list of indices"
      ],
      "metadata": {
        "id": "MnWj6-V2aXXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab.lookup_tokens([0,5,13])"
      ],
      "metadata": {
        "id": "YwZCfgZX8xK_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea2d8c9f-12d6-4a8b-dd47-d3f0d9900392"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<unk>', 'brown', 'arrive']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Could tokenise on other sentences based on the vocab created\n",
        "- Prepare the text processing pipeline with the tokenizer and vocabulary\n",
        "- The text pipeline converts a text string into a list of integers based on the lookup table defined in the vocabulary. \n",
        "- The label pipeline converts the label into integers."
      ],
      "metadata": {
        "id": "0yIZTeJGvcoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "label_pipeline = lambda x: int(x) - 1\n",
        "## 이부분 잘 이해가 안됨\n"
      ],
      "metadata": {
        "id": "0U7kIT5AvhaQ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_pipeline('here is a warm example')"
      ],
      "metadata": {
        "id": "JJ0aSPG-voHe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21175672-a593-424c-a6fc-d8c9e328d233"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 24, 12, 0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q5. Explain what each integer in the list returned by text_pipeline means.\n",
        "# A5. It means the index of each word. \n",
        "# 0 means unknow vocab meaning that there was no such word in the original dictionary"
      ],
      "metadata": {
        "id": "hNHLgycWxpas"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fHdeTGoI8q6"
      },
      "source": [
        "## 2.2. Reading movie reviews from file\n",
        "- Adapted from [HERE](https://jamesmccaffrey.wordpress.com/2021/06/18/serving-up-pytorch-training-data-using-the-dataloader-collate_fn-parameter/).\n",
        "- The file contains the review score (0 for negative, 1 for positive) and the actual review in each line, separated by a comma.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the small text file containing the movie reviews\n",
        "!wget 'https://docs.google.com/uc?export=download&id=1QjyaIEL4H4opnPh9NhLcfU6Ws8qGC2kR' -O reviews.txt\n"
      ],
      "metadata": {
        "id": "HE8bTdbYgfxY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11db0dab-83bc-4543-9a6a-38b44a91afe5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-09-17 06:53:52--  https://docs.google.com/uc?export=download&id=1QjyaIEL4H4opnPh9NhLcfU6Ws8qGC2kR\n",
            "Resolving docs.google.com (docs.google.com)... 142.250.141.102, 142.250.141.101, 142.250.141.100, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.250.141.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-14-a0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/ekduasi0ab1j6v7st73vpfmdt01drh4q/1663397625000/18412391637135455491/*/1QjyaIEL4H4opnPh9NhLcfU6Ws8qGC2kR?e=download&uuid=b49b10aa-b7a8-43b0-bdc0-313058b849b8 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-09-17 06:53:52--  https://doc-14-a0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/ekduasi0ab1j6v7st73vpfmdt01drh4q/1663397625000/18412391637135455491/*/1QjyaIEL4H4opnPh9NhLcfU6Ws8qGC2kR?e=download&uuid=b49b10aa-b7a8-43b0-bdc0-313058b849b8\n",
            "Resolving doc-14-a0-docs.googleusercontent.com (doc-14-a0-docs.googleusercontent.com)... 142.250.141.132, 2607:f8b0:4023:c0b::84\n",
            "Connecting to doc-14-a0-docs.googleusercontent.com (doc-14-a0-docs.googleusercontent.com)|142.250.141.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 298 [text/plain]\n",
            "Saving to: ‘reviews.txt’\n",
            "\n",
            "reviews.txt         100%[===================>]     298  --.-KB/s    in 0s      \n",
            "\n",
            "2022-09-17 06:53:53 (15.3 MB/s) - ‘reviews.txt’ saved [298/298]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q6. Locate the name of this file and where it is saved\n",
        "# A6. /content/reviews.txt\n",
        "\n",
        "# 확인해보기"
      ],
      "metadata": {
        "id": "Hr2eW7mMibQ-"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJn5RR6DJAS-"
      },
      "source": [
        "import torch\n",
        "import torchtext as tt\n",
        "import collections\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# data file (review.txt) looks like:\n",
        "# 0, This was a BAD movie.\n",
        "# 1, I liked this film! Highly recommeneded.\n",
        "# 0, Don't waste your time - a real dud\n",
        "# 1, Good film. Great acting.\n",
        "# 0, This was a waste of talent.\n",
        "# 1, Great movie. Good acting.\n",
        "# ..."
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMPdV-ubJP9p"
      },
      "source": [
        "Make Vocab\n",
        "- Need to tokenise on the review words\n",
        "- Need to format the text in the file\n",
        "- Tokenise the review text using `get_tokenizer()`  and build the vocab using `build_vocab_from_iterator()`.\n",
        "- We will use [`collections.Counter()`](https://docs.python.org/3/library/collections.html#collections.Counter) class to hold the tokenised review and passed to `build_vocab_from_iterator()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsBy_THIJSPV"
      },
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "def yield_tok(counter_obj, tokenizer):\n",
        "  for c in counter_obj:\n",
        "    yield tokenizer(c)\n",
        "\n",
        "def make_vocab(fn):\n",
        "  tokeniser = get_tokenizer(\"basic_english\")  # local\n",
        "  counter_obj = collections.Counter()\n",
        "  f = open(fn, \"r\") # open file for reading\n",
        "  for line in f:\n",
        "    line = line.strip() # all leading and trailing whitespaces are removed from the string\n",
        "    txt = line.split(\",\")[1] # split line by \",\" and return item at index 1\n",
        "    split_and_lowered = tokeniser(txt)\n",
        "    counter_obj.update(split_and_lowered)\n",
        "  f.close()\n",
        "  token_generator = yield_tok(counter_obj, tokeniser) # build_vocab_from_iterator() requires an iterable wrapping an iterable containing words/tokens.\n",
        "\n",
        "  vocab = build_vocab_from_iterator(token_generator, specials=[\"<unk>\"], min_freq=1)\n",
        "  vocab.set_default_index(vocab[\"<unk>\"])\n",
        "\n",
        "  print(f\"Vocab: {vocab.get_stoi()}\")\n",
        "  return vocab"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = make_vocab('reviews.txt')\n",
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmMOiyJOS3QF",
        "outputId": "7979e600-3bd8-419f-fb4e-3d03dcaca6a4"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab: {'your': 40, 'would': 39, 'dud': 12, 'good': 15, 'for': 14, 'don': 11, 'highly': 17, 'recommended': 28, 'penny': 26, 'spend': 30, 'not': 23, 'waste': 38, 'afternoon': 6, 'bad': 10, '!': 1, '<unk>': 0, 'real': 27, 'awful': 9, \"'\": 2, '-': 3, 'time': 36, 'film': 13, 'all': 7, '.': 4, 't': 32, 'at': 8, 'great': 16, 'movie': 21, 'i': 18, 'talent': 33, 'just': 19, 'was': 37, 'liked': 20, 'this': 35, 'of': 24, 'a': 5, 'sunday': 31, 'on': 25, 'nap': 22, 'single': 29, 'terrible': 34}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Vocab()"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab.lookup_tokens([1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfozcOetTLWV",
        "outputId": "2c1c73fd-9bcc-41e1-a635-f86b694ea0ea"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['!']"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q7. Why is the item at index 1 returned by the line split?\n",
        "# "
      ],
      "metadata": {
        "id": "GJRZA2WAmReT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zzwvolj1JWod",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "301dc71d-1cb5-48ec-a5d1-f05202b38352"
      },
      "source": [
        "# globals are needed for the collate_fn() function\n",
        "g_tokenizer = get_tokenizer(\"basic_english\")  # global tokenizer\n",
        "g_vocab = make_vocab(\"./reviews.txt\")  # global vocabulary\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab: {'your': 40, 'would': 39, 'dud': 12, 'good': 15, 'for': 14, 'don': 11, 'highly': 17, 'recommended': 28, 'penny': 26, 'spend': 30, 'not': 23, 'waste': 38, 'afternoon': 6, 'bad': 10, '!': 1, '<unk>': 0, 'real': 27, 'awful': 9, \"'\": 2, '-': 3, 'time': 36, 'film': 13, 'all': 7, '.': 4, 't': 32, 'at': 8, 'great': 16, 'movie': 21, 'i': 18, 'talent': 33, 'just': 19, 'was': 37, 'liked': 20, 'this': 35, 'of': 24, 'a': 5, 'sunday': 31, 'on': 25, 'nap': 22, 'single': 29, 'terrible': 34}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q8. How would you get a test string's tokens using g_tokenizer and g_vocab? (CODE)\n"
      ],
      "metadata": {
        "id": "YkJfSq8ExWEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd81ytqEMTHf"
      },
      "source": [
        "Prepare Data for DataLoader\n",
        "- Create list of tuples from each line in file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "me3GtotfJd8f"
      },
      "source": [
        "def make_data_list(fname):\n",
        "  # get all data into one big list of (label, review) tuples\n",
        "  # result will be passed to DataLoader\n",
        "  result = []\n",
        "  f = open(fname, \"r\")\n",
        "  for line in f:\n",
        "    line = line.strip()\n",
        "    parts = line.split(\",\")\n",
        "    tuple = (parts[0], parts[1]) # Create tuple\n",
        "    result.append(tuple)\n",
        "  f.close()\n",
        "  return result "
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XP2uEr4CMNoi"
      },
      "source": [
        "Custom Collate Function\n",
        "- Convert labels to int and tensor\n",
        "- Tokenise review text and convert to tensor\n",
        "- Get offset indexes of each word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFJBuOU9Jils"
      },
      "source": [
        "# rearrange a batch and compute offsets\n",
        "# needs a global vocab and tokenizer\n",
        "def collate_data(batch):\n",
        "  label_list, review_list, offset_list = [], [], [0]\n",
        "  # print(\"\\n\")\n",
        "  for (label, review) in batch:  # 2 items in each item of batch\n",
        "    label_list.append(int(label))  # string to int, then append to a list\n",
        "    r_idxs = [g_vocab[tok] for tok in g_tokenizer(review)]  # list of tokens\n",
        "    r_idxs = torch.tensor(r_idxs, dtype=torch.int64)  # to tensor\n",
        "    review_list.append(r_idxs)\n",
        "    print(f'Review: {review}')\n",
        "    offset_list.append(len(r_idxs)) # get the index of the next word and put it in offset_list\n",
        "\n",
        "  print(f\"Label list {label_list}\")\n",
        "  label_list = torch.tensor(label_list, dtype=torch.int64).to(device)  # convert to tensor\n",
        "  print(f'Offsets before cumsum: {offset_list}')\n",
        "  offset_list = torch.tensor(offset_list[:-1]).cumsum(dim=0).to(device)    # whoa!\n",
        "  print(f'Offsets AFTER cumsum: {offset_list}')\n",
        "  review_list = torch.cat(review_list).to(device)  # combine 2 tensors into 1\n",
        "\n",
        "  return (label_list, review_list, offset_list)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quick illustration of [`cumsum()`](https://numpy.org/doc/stable/reference/generated/numpy.cumsum.html)"
      ],
      "metadata": {
        "id": "9Qt2-CGg1Gcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.arange(0, 6).view(2, 3)\n",
        "print(x)\n",
        "print(f'Cumulative sum in 1st dim: \\n{x.cumsum(dim=0)}')\n",
        "print(f'Cumulative sum in 2nd dim: \\n{x.cumsum(dim=1)}')"
      ],
      "metadata": {
        "id": "ALA6Dohp1FXe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09abe450-11db-4c11-d7f9-c749f7a0b6f4"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0, 1, 2],\n",
            "        [3, 4, 5]])\n",
            "Cumulative sum in 1st dim: \n",
            "tensor([[0, 1, 2],\n",
            "        [3, 5, 7]])\n",
            "Cumulative sum in 2nd dim: \n",
            "tensor([[ 0,  1,  3],\n",
            "        [ 3,  7, 12]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IY9GUrrKMaAj"
      },
      "source": [
        "### Call all the functions\n",
        "- Prepare data from file\n",
        "- Feed data to DataLoader\n",
        "- Print the batches\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdsjfuZ5Jsrv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60ee01f5-9bd4-40ff-9b1f-e2416f795a2a"
      },
      "source": [
        "batch_size = 3\n",
        "print(\"Begin DataLoader demo using text from file\")\n",
        "\n",
        "print(\"\\nLoading train data into tuples: \")\n",
        "data_list = make_data_list(\"./reviews.txt\")\n",
        "print(data_list)\n",
        "\n",
        "print(\"\\nCreating DataLoader from tuples \")\n",
        "train_loader = torch.utils.data.DataLoader(data_list, \\\n",
        "  batch_size=batch_size, shuffle=False, collate_fn=collate_data)\n",
        "\n",
        "print(\"\\nWorking with batches (size = 3): \")\n",
        "for b_ix, (labels, reviews, offsets) in enumerate(train_loader):\n",
        "  print(\"==========\")\n",
        "  print(f\"BATCH  : {b_ix}\")\n",
        "  print(\"Labels : \", end=\"\"); print(labels)\n",
        "  print(\"Reviews: \", end=\"\"); print(reviews)\n",
        "  print(\"Offsets: \", end=\"\"); print(offsets)\n",
        "  print(\"====================================\\n\\n\")\n",
        "\n",
        "print(\"\\nEnd demo\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Begin DataLoader demo using text from file\n",
            "\n",
            "Loading train data into tuples: \n",
            "[('0', ' This was a BAD movie.'), ('1', ' I liked this film! Highly recommended.'), ('0', ' Just awful'), ('1', ' Good film'), ('0', \" Don't waste your time - A real dud\"), ('0', ' Terrible!'), ('1', ' Great movie.'), ('0', ' This was a waste of talent.'), ('1', ' Not bad at all.'), ('0', ' Would not spend a single penny on this'), ('0', ' Not bad for a Sunday afternoon nap')]\n",
            "\n",
            "Creating DataLoader from tuples \n",
            "\n",
            "Working with batches (size = 3): \n",
            "Review:  This was a BAD movie.\n",
            "Review:  I liked this film! Highly recommended.\n",
            "Review:  Just awful\n",
            "Label list [0, 1, 0]\n",
            "Offsets before cumsum: [0, 6, 8, 2]\n",
            "Offsets AFTER cumsum: tensor([ 0,  6, 14])\n",
            "==========\n",
            "BATCH  : 0\n",
            "Labels : tensor([0, 1, 0])\n",
            "Reviews: tensor([35, 37,  5, 10, 21,  4, 18, 20, 35, 13,  1, 17, 28,  4, 19,  9])\n",
            "Offsets: tensor([ 0,  6, 14])\n",
            "====================================\n",
            "\n",
            "\n",
            "Review:  Good film\n",
            "Review:  Don't waste your time - A real dud\n",
            "Review:  Terrible!\n",
            "Label list [1, 0, 0]\n",
            "Offsets before cumsum: [0, 2, 10, 2]\n",
            "Offsets AFTER cumsum: tensor([ 0,  2, 12])\n",
            "==========\n",
            "BATCH  : 1\n",
            "Labels : tensor([1, 0, 0])\n",
            "Reviews: tensor([15, 13, 11,  2, 32, 38, 40, 36,  3,  5, 27, 12, 34,  1])\n",
            "Offsets: tensor([ 0,  2, 12])\n",
            "====================================\n",
            "\n",
            "\n",
            "Review:  Great movie.\n",
            "Review:  This was a waste of talent.\n",
            "Review:  Not bad at all.\n",
            "Label list [1, 0, 1]\n",
            "Offsets before cumsum: [0, 3, 7, 5]\n",
            "Offsets AFTER cumsum: tensor([ 0,  3, 10])\n",
            "==========\n",
            "BATCH  : 2\n",
            "Labels : tensor([1, 0, 1])\n",
            "Reviews: tensor([16, 21,  4, 35, 37,  5, 38, 24, 33,  4, 23, 10,  8,  7,  4])\n",
            "Offsets: tensor([ 0,  3, 10])\n",
            "====================================\n",
            "\n",
            "\n",
            "Review:  Would not spend a single penny on this\n",
            "Review:  Not bad for a Sunday afternoon nap\n",
            "Label list [0, 0]\n",
            "Offsets before cumsum: [0, 8, 7]\n",
            "Offsets AFTER cumsum: tensor([0, 8])\n",
            "==========\n",
            "BATCH  : 3\n",
            "Labels : tensor([0, 0])\n",
            "Reviews: tensor([39, 23, 30,  5, 29, 26, 25, 35, 23, 10, 14,  5, 31,  6, 22])\n",
            "Offsets: tensor([0, 8])\n",
            "====================================\n",
            "\n",
            "\n",
            "\n",
            "End demo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q9. Based on the output, explain what this line of code does in collate_fn():\n",
        "# offset_list = torch.tensor(offset_list[:-1]).cumsum(dim=0).to(device)\n"
      ],
      "metadata": {
        "id": "w9EaurKK-LiT"
      },
      "execution_count": 31,
      "outputs": []
    }
  ]
}